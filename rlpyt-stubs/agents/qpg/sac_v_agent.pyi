from collections import namedtuple
from rlpyt.agents.base import AgentStep as AgentStep, BaseAgent as BaseAgent
from rlpyt.distributions.gaussian import DistInfoStd as DistInfoStd, Gaussian as Gaussian
from rlpyt.models.qpg.mlp import PiMlpModel as PiMlpModel, QofMuMlpModel as QofMuMlpModel, VMlpModel as VMlpModel
from rlpyt.models.utils import update_state_dict as update_state_dict
from rlpyt.utils.buffer import buffer_to as buffer_to
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from typing import Any, Optional

MIN_LOG_STD: int
MAX_LOG_STD: int
AgentInfo: Any

Models = namedtuple('Models', ['pi', 'q1', 'q2', 'v'])

class SacAgent(BaseAgent):
    min_itr_learn: int = ...
    def __init__(self, ModelCls: Any = ..., QModelCls: Any = ..., VModelCls: Any = ..., model_kwargs: Optional[Any] = ..., q_model_kwargs: Optional[Any] = ..., v_model_kwargs: Optional[Any] = ..., initial_model_state_dict: Optional[Any] = ..., action_squash: float = ..., pretrain_std: float = ...) -> None: ...
    initial_model_state_dict: Any = ...
    q1_model: Any = ...
    q2_model: Any = ...
    v_model: Any = ...
    target_v_model: Any = ...
    distribution: Any = ...
    def initialize(self, env_spaces: Any, share_memory: bool = ..., global_B: int = ..., env_ranks: Optional[Any] = ...) -> None: ...
    def to_device(self, cuda_idx: Optional[Any] = ...) -> None: ...
    def data_parallel(self): ...
    def give_min_itr_learn(self, min_itr_learn: Any) -> None: ...
    def make_env_to_model_kwargs(self, env_spaces: Any): ...
    def q(self, observation: Any, prev_action: Any, prev_reward: Any, action: Any): ...
    def v(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def pi(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def target_v(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def step(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def update_target(self, tau: int = ...) -> None: ...
    @property
    def models(self): ...
    def pi_parameters(self): ...
    def q1_parameters(self): ...
    def q2_parameters(self): ...
    def v_parameters(self): ...
    def train_mode(self, itr: Any) -> None: ...
    def sample_mode(self, itr: Any) -> None: ...
    def eval_mode(self, itr: Any) -> None: ...
    def state_dict(self): ...
    def load_state_dict(self, state_dict: Any) -> None: ...
