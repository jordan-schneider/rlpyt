# Stubs for rlpyt.agents.qpg.ddpg_agent (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from rlpyt.agents.base import BaseAgent
from typing import Any, Optional

AgentInfo: Any

class DdpgAgent(BaseAgent):
    shared_mu_model: Any = ...
    def __init__(self, ModelCls: Any = ..., QModelCls: Any = ..., model_kwargs: Optional[Any] = ..., q_model_kwargs: Optional[Any] = ..., initial_model_state_dict: Optional[Any] = ..., initial_q_model_state_dict: Optional[Any] = ..., action_std: float = ..., action_noise_clip: Optional[Any] = ...) -> None: ...
    q_model: Any = ...
    target_model: Any = ...
    target_q_model: Any = ...
    distribution: Any = ...
    def initialize(self, env_spaces: Any, share_memory: bool = ..., global_B: int = ..., env_ranks: Optional[Any] = ...) -> None: ...
    def to_device(self, cuda_idx: Optional[Any] = ...) -> None: ...
    def data_parallel(self) -> None: ...
    def make_env_to_model_kwargs(self, env_spaces: Any): ...
    def q(self, observation: Any, prev_action: Any, prev_reward: Any, action: Any): ...
    def q_at_mu(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def target_q_at_mu(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def step(self, observation: Any, prev_action: Any, prev_reward: Any): ...
    def update_target(self, tau: int = ...) -> None: ...
    def q_parameters(self): ...
    def mu_parameters(self): ...
    def train_mode(self, itr: Any) -> None: ...
    def sample_mode(self, itr: Any) -> None: ...
    def eval_mode(self, itr: Any) -> None: ...
    def state_dict(self): ...
    def load_state_dict(self, state_dict: Any) -> None: ...
