from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm as RlAlgorithm
from rlpyt.algos.utils import valid_from_done as valid_from_done
from rlpyt.distributions.gaussian import Gaussian as Gaussian
from rlpyt.replays.non_sequence.time_limit import TlUniformReplayBuffer as TlUniformReplayBuffer
from rlpyt.replays.non_sequence.uniform import UniformReplayBuffer as UniformReplayBuffer
from rlpyt.ul.algos.utils.data_augs import random_shift as random_shift, subpixel_shift as subpixel_shift
from rlpyt.utils.buffer import buffer_to as buffer_to
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import valid_mean as valid_mean
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['q1Loss', 'q2Loss', 'piLoss', 'qGradNorm', 'piGradNorm', 'q1', 'q2', 'piMu', 'piLogStd', 'qMeanDiff', 'alpha'])
SamplesToBuffer: Any
SamplesToBufferTl: Any

def chain(*iterables: Any) -> None: ...

class RadSacFromUl(RlAlgorithm):
    opt_info_fields: Any = ...
    replay_ratio: Any = ...
    def __init__(self, discount: float = ..., batch_size: int = ..., min_steps_learn: Any = ..., replay_size: Any = ..., target_update_tau: float = ..., target_update_interval: int = ..., actor_update_interval: int = ..., OptimCls: Any = ..., initial_optim_state_dict: Optional[Any] = ..., action_prior: str = ..., reward_scale: int = ..., target_entropy: str = ..., reparameterize: bool = ..., clip_grad_norm: float = ..., n_step_return: int = ..., bootstrap_timelimit: bool = ..., q_lr: float = ..., pi_lr: float = ..., alpha_lr: float = ..., q_beta: float = ..., pi_beta: float = ..., alpha_beta: float = ..., alpha_init: float = ..., encoder_update_tau: float = ..., augmentation: str = ..., random_shift_pad: int = ..., random_shift_prob: float = ..., stop_conv_grad: bool = ..., max_pixel_shift: float = ...) -> None: ...
    agent: Any = ...
    n_itr: Any = ...
    mid_batch_reset: Any = ...
    sampler_bs: Any = ...
    updates_per_optimize: Any = ...
    min_itr_learn: Any = ...
    store_latent: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ..., rank: int = ...) -> None: ...
    def async_initialize(*args: Any, **kwargs: Any) -> None: ...
    rank: Any = ...
    pi_optimizer: Any = ...
    q_optimizer: Any = ...
    alpha_optimizer: Any = ...
    target_entropy: Any = ...
    action_prior_distribution: Any = ...
    def optim_initialize(self, rank: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...) -> None: ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def examples_to_buffer(self, examples: Any): ...
    def samples_to_device(self, samples: Any): ...
    def data_aug_loss_samples(self, samples: Any): ...
    def q_loss(self, samples: Any): ...
    def pi_alpha_loss(self, samples: Any, valid: Any, conv_out: Any): ...
    def get_action_prior(self, action: Any): ...
    def optim_state_dict(self): ...
    def load_optim_state_dict(self, state_dict: Any) -> None: ...
