from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm as RlAlgorithm
from rlpyt.algos.utils import valid_from_done as valid_from_done
from rlpyt.distributions.gaussian import Gaussian as Gaussian
from rlpyt.models.utils import update_state_dict as update_state_dict
from rlpyt.replays.non_sequence.time_limit import TlUniformReplayBuffer as TlUniformReplayBuffer
from rlpyt.replays.non_sequence.uniform import UniformReplayBuffer as UniformReplayBuffer
from rlpyt.ul.algos.utils.data_augs import random_shift as random_shift
from rlpyt.ul.algos.utils.warmup_scheduler import GradualWarmupScheduler as GradualWarmupScheduler
from rlpyt.ul.models.rl.ul_models import UlEncoderModel as UlEncoderModel
from rlpyt.ul.models.ul.atc_models import ContrastModel as ContrastModel
from rlpyt.ul.replays.rl_with_ul_replay import RlWithUlPrioritizedReplayWrapper as RlWithUlPrioritizedReplayWrapper
from rlpyt.utils.buffer import buffer_to as buffer_to
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import valid_mean as valid_mean
from typing import Any, Optional

IGNORE_INDEX: int

OptInfoUl = namedtuple('OptInfoUl', ['ulLoss', 'ulAccuracy', 'ulGradNorm', 'ulUpdates'])

OptInfoRl = namedtuple('OptInfoRl', ['q1Loss', 'q2Loss', 'piLoss', 'qGradNorm', 'piGradNorm', 'q1', 'q2', 'piMu', 'piLogStd', 'qMeanDiff', 'alpha'])

OptInfo: AnySamplesToBuffer: Any
SamplesToBufferTl: Any

def chain(*iterables: Any) -> None: ...

class SacWithUl(RlAlgorithm):
    opt_info_fields: Any = ...
    replay_ratio: Any = ...
    def __init__(self, discount: float = ..., batch_size: int = ..., replay_size: Any = ..., target_update_tau: float = ..., target_update_interval: int = ..., actor_update_interval: int = ..., OptimCls: Any = ..., initial_optim_state_dict: Optional[Any] = ..., action_prior: str = ..., reward_scale: int = ..., target_entropy: str = ..., reparameterize: bool = ..., clip_grad_norm: float = ..., n_step_return: int = ..., bootstrap_timelimit: bool = ..., q_lr: float = ..., pi_lr: float = ..., alpha_lr: float = ..., q_beta: float = ..., pi_beta: float = ..., alpha_beta: float = ..., alpha_init: float = ..., encoder_update_tau: float = ..., random_shift_prob: float = ..., random_shift_pad: int = ..., stop_rl_conv_grad: bool = ..., min_steps_rl: Any = ..., min_steps_ul: Any = ..., max_steps_ul: Optional[Any] = ..., ul_learning_rate: float = ..., ul_optim_kwargs: Optional[Any] = ..., ul_update_schedule: Optional[Any] = ..., ul_lr_schedule: Optional[Any] = ..., ul_lr_warmup: int = ..., ul_batch_size: int = ..., ul_random_shift_prob: float = ..., ul_random_shift_pad: int = ..., ul_target_update_interval: int = ..., ul_target_update_tau: float = ..., ul_latent_size: int = ..., ul_anchor_hidden_sizes: int = ..., ul_clip_grad_norm: float = ..., ul_pri_alpha: float = ..., ul_pri_beta: float = ..., ul_pri_n_step_return: int = ..., ul_use_rl_samples: bool = ..., UlEncoderCls: Any = ..., UlContrastCls: Any = ...) -> None: ...
    agent: Any = ...
    n_itr: Any = ...
    mid_batch_reset: Any = ...
    sampler_bs: Any = ...
    updates_per_optimize: Any = ...
    min_itr_rl: Any = ...
    min_itr_ul: Any = ...
    max_itr_ul: Any = ...
    ul_encoder: Any = ...
    ul_target_encoder: Any = ...
    ul_contrast: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ..., rank: int = ...) -> None: ...
    def async_initialize(*args: Any, **kwargs: Any) -> None: ...
    rank: Any = ...
    pi_optimizer: Any = ...
    q_optimizer: Any = ...
    alpha_optimizer: Any = ...
    target_entropy: Any = ...
    action_prior_distribution: Any = ...
    ul_optimizer: Any = ...
    total_ul_updates: Any = ...
    ul_update_counter: int = ...
    ul_lr_scheduler: Any = ...
    c_e_loss: Any = ...
    def optim_initialize(self, rank: int = ...): ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...) -> None: ...
    def optimize_agent(self, itr: Any, samples: Any): ...
    def rl_optimize(self, itr: Any): ...
    def ul_optimize(self, itr: Any, rl_samples: Optional[Any] = ...): ...
    def ul_optimize_one_step(self, samples: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def examples_to_buffer(self, examples: Any): ...
    def samples_to_device(self, samples: Any): ...
    def random_shift_rl_samples(self, samples: Any): ...
    def q_loss(self, samples: Any): ...
    def pi_alpha_loss(self, samples: Any, valid: Any, conv_out: Any): ...
    def get_action_prior(self, action: Any): ...
    def optim_state_dict(self): ...
    def load_optim_state_dict(self, state_dict: Any) -> None: ...
    def ul_parameters(self) -> None: ...
    def ul_named_parameters(self) -> None: ...
    def compute_ul_update_schedule(self, itr: Any): ...
