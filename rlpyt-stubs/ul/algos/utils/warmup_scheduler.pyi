from torch.optim.lr_scheduler import _LRScheduler
from typing import Any, Optional

class GradualWarmupScheduler(_LRScheduler):
    multiplier: Any = ...
    total_epoch: Any = ...
    after_scheduler: Any = ...
    finished: bool = ...
    def __init__(self, optimizer: Any, multiplier: Any, total_epoch: Any, after_scheduler: Optional[Any] = ...) -> None: ...
    def get_lr(self): ...
    def step(self, epoch: Optional[Any] = ..., metrics: Optional[Any] = ...): ...
