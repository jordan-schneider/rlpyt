from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm as RlAlgorithm
from rlpyt.algos.utils import valid_from_done as valid_from_done
from rlpyt.replays.non_sequence.time_limit import AsyncTlUniformReplayBuffer as AsyncTlUniformReplayBuffer, TlUniformReplayBuffer as TlUniformReplayBuffer
from rlpyt.replays.non_sequence.uniform import AsyncUniformReplayBuffer as AsyncUniformReplayBuffer, UniformReplayBuffer as UniformReplayBuffer
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import valid_mean as valid_mean
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['muLoss', 'qLoss', 'muGradNorm', 'qGradNorm'])
SamplesToBuffer: Any

class DDPG(RlAlgorithm):
    opt_info_fields: Any = ...
    def __init__(self, discount: float = ..., batch_size: int = ..., min_steps_learn: Any = ..., replay_size: Any = ..., replay_ratio: int = ..., target_update_tau: float = ..., target_update_interval: int = ..., policy_update_interval: int = ..., learning_rate: float = ..., q_learning_rate: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., initial_optim_state_dict: Optional[Any] = ..., clip_grad_norm: float = ..., q_target_clip: float = ..., n_step_return: int = ..., updates_per_sync: int = ..., bootstrap_timelimit: bool = ..., ReplayBufferCls: Optional[Any] = ...) -> None: ...
    agent: Any = ...
    n_itr: Any = ...
    mid_batch_reset: Any = ...
    sampler_bs: Any = ...
    updates_per_optimize: Any = ...
    min_itr_learn: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ..., rank: int = ...) -> None: ...
    def async_initialize(self, agent: Any, sampler_n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ...): ...
    rank: Any = ...
    mu_optimizer: Any = ...
    q_optimizer: Any = ...
    def optim_initialize(self, rank: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...) -> None: ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def mu_loss(self, samples: Any, valid: Any): ...
    def q_loss(self, samples: Any, valid: Any): ...
    def optim_state_dict(self): ...
    def load_optim_state_dict(self, state_dict: Any) -> None: ...
