# Stubs for rlpyt.algos.qpg.sac_v (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['q1Loss', 'q2Loss', 'vLoss', 'piLoss', 'q1GradNorm', 'q2GradNorm', 'vGradNorm', 'piGradNorm', 'q1', 'q2', 'v', 'piMu', 'piLogStd', 'qMeanDiff'])
SamplesToBuffer: Any

class SAC_V(RlAlgorithm):
    opt_info_fields: Any = ...
    def __init__(self, discount: float = ..., batch_size: int = ..., min_steps_learn: Any = ..., replay_size: Any = ..., replay_ratio: int = ..., target_update_tau: float = ..., target_update_interval: int = ..., learning_rate: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., initial_optim_state_dict: Optional[Any] = ..., action_prior: str = ..., reward_scale: int = ..., reparameterize: bool = ..., clip_grad_norm: float = ..., policy_output_regularization: float = ..., n_step_return: int = ..., updates_per_sync: int = ..., bootstrap_timelimit: bool = ...) -> None: ...
    agent: Any = ...
    n_itr: Any = ...
    mid_batch_reset: Any = ...
    sampler_bs: Any = ...
    updates_per_optimize: Any = ...
    min_itr_learn: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ..., rank: int = ...) -> None: ...
    def async_initialize(self, agent: Any, sampler_n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ...): ...
    rank: Any = ...
    pi_optimizer: Any = ...
    q1_optimizer: Any = ...
    q2_optimizer: Any = ...
    v_optimizer: Any = ...
    action_prior_distribution: Any = ...
    def optim_initialize(self, rank: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...) -> None: ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def loss(self, samples: Any): ...
    def get_action_prior(self, action: Any): ...
    def append_opt_info_(self, opt_info: Any, losses: Any, grad_norms: Any, values: Any) -> None: ...
    def optim_state_dict(self): ...
    def load_optim_state_dict(self, state_dict: Any) -> None: ...
