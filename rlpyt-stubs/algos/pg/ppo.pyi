from rlpyt.agents.base import AgentInputs as AgentInputs, AgentInputsRnn as AgentInputsRnn
from rlpyt.algos.pg.base import OptInfo as OptInfo, PolicyGradientAlgo as PolicyGradientAlgo
from rlpyt.utils.buffer import buffer_method as buffer_method, buffer_to as buffer_to
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.misc import iterate_mb_idxs as iterate_mb_idxs
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import valid_mean as valid_mean
from typing import Any, Optional

LossInputs: Any

class PPO(PolicyGradientAlgo):
    def __init__(self, discount: float = ..., learning_rate: float = ..., value_loss_coeff: float = ..., entropy_loss_coeff: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., clip_grad_norm: float = ..., initial_optim_state_dict: Optional[Any] = ..., gae_lambda: int = ..., minibatches: int = ..., epochs: int = ..., ratio_clip: float = ..., linear_lr_schedule: bool = ..., normalize_advantage: bool = ...) -> None: ...
    lr_scheduler: Any = ...
    def initialize(self, *args: Any, **kwargs: Any): ...
    ratio_clip: Any = ...
    def optimize_agent(self, itr: Any, samples: Any): ...
    def loss(self, agent_inputs: Any, action: Any, return_: Any, advantage: Any, valid: Any, old_dist_info: Any, init_rnn_state: Optional[Any] = ...): ...
