from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm as RlAlgorithm
from rlpyt.algos.utils import discount_return as discount_return, generalized_advantage_estimation as generalized_advantage_estimation, valid_from_done as valid_from_done
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['loss', 'gradNorm', 'entropy', 'perplexity'])

AgentTrain = namedtuple('AgentTrain', ['dist_info', 'value'])

class PolicyGradientAlgo(RlAlgorithm):
    bootstrap_value: bool = ...
    opt_info_fields: Any = ...
    optimizer: Any = ...
    agent: Any = ...
    n_itr: Any = ...
    batch_spec: Any = ...
    mid_batch_reset: Any = ...
    rank: Any = ...
    world_size: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: bool = ..., examples: Optional[Any] = ..., world_size: int = ..., rank: int = ...) -> None: ...
    def process_returns(self, samples: Any): ...
