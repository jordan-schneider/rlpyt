from rlpyt.agents.base import AgentInputs as AgentInputs
from rlpyt.algos.pg.base import OptInfo as OptInfo, PolicyGradientAlgo as PolicyGradientAlgo
from rlpyt.utils.buffer import buffer_method as buffer_method
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import valid_mean as valid_mean
from typing import Any, Optional

class A2C(PolicyGradientAlgo):
    def __init__(self, discount: float = ..., learning_rate: float = ..., value_loss_coeff: float = ..., entropy_loss_coeff: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., clip_grad_norm: float = ..., initial_optim_state_dict: Optional[Any] = ..., gae_lambda: int = ..., normalize_advantage: bool = ...) -> None: ...
    def initialize(self, *args: Any, **kwargs: Any) -> None: ...
    def optimize_agent(self, itr: Any, samples: Any): ...
    def loss(self, samples: Any): ...
