from collections import namedtuple
from rlpyt.algos.base import RlAlgorithm as RlAlgorithm
from rlpyt.algos.utils import valid_from_done as valid_from_done
from rlpyt.replays.non_sequence.frame import AsyncPrioritizedReplayFrameBuffer as AsyncPrioritizedReplayFrameBuffer, AsyncUniformReplayFrameBuffer as AsyncUniformReplayFrameBuffer, PrioritizedReplayFrameBuffer as PrioritizedReplayFrameBuffer, UniformReplayFrameBuffer as UniformReplayFrameBuffer
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import select_at_indexes as select_at_indexes, valid_mean as valid_mean
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['loss', 'gradNorm', 'tdAbsErr'])
SamplesToBuffer: Any

class DQN(RlAlgorithm):
    opt_info_fields: Any = ...
    update_counter: int = ...
    def __init__(self, discount: float = ..., batch_size: int = ..., min_steps_learn: Any = ..., delta_clip: float = ..., replay_size: Any = ..., replay_ratio: int = ..., target_update_tau: int = ..., target_update_interval: int = ..., n_step_return: int = ..., learning_rate: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., initial_optim_state_dict: Optional[Any] = ..., clip_grad_norm: float = ..., eps_steps: Any = ..., double_dqn: bool = ..., prioritized_replay: bool = ..., pri_alpha: float = ..., pri_beta_init: float = ..., pri_beta_final: float = ..., pri_beta_steps: Any = ..., default_priority: Optional[Any] = ..., ReplayBufferCls: Optional[Any] = ..., updates_per_sync: int = ...) -> None: ...
    agent: Any = ...
    n_itr: Any = ...
    sampler_bs: Any = ...
    mid_batch_reset: Any = ...
    updates_per_optimize: Any = ...
    min_itr_learn: Any = ...
    def initialize(self, agent: Any, n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ..., rank: int = ...) -> None: ...
    def async_initialize(self, agent: Any, sampler_n_itr: Any, batch_spec: Any, mid_batch_reset: Any, examples: Any, world_size: int = ...): ...
    rank: Any = ...
    optimizer: Any = ...
    pri_beta_itr: Any = ...
    def optim_initialize(self, rank: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...) -> None: ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def examples_to_buffer(self, examples: Any): ...
    def samples_to_buffer(self, samples: Any): ...
    def loss(self, samples: Any): ...
    def update_itr_hyperparams(self, itr: Any) -> None: ...
