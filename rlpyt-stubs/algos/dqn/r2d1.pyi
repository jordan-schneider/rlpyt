# Stubs for rlpyt.algos.dqn.r2d1 (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from collections import namedtuple
from rlpyt.algos.dqn.dqn import DQN
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['loss', 'gradNorm', 'tdAbsErr', 'priority'])
SamplesToBufferRnn: Any
PrioritiesSamplesToBuffer: Any

class R2D1(DQN):
    opt_info_fields: Any = ...
    def __init__(self, discount: float = ..., batch_T: int = ..., batch_B: int = ..., warmup_T: int = ..., store_rnn_state_interval: int = ..., min_steps_learn: Any = ..., delta_clip: Optional[Any] = ..., replay_size: Any = ..., replay_ratio: int = ..., target_update_interval: int = ..., n_step_return: int = ..., learning_rate: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., initial_optim_state_dict: Optional[Any] = ..., clip_grad_norm: float = ..., eps_steps: Any = ..., double_dqn: bool = ..., prioritized_replay: bool = ..., pri_alpha: float = ..., pri_beta_init: float = ..., pri_beta_final: float = ..., pri_beta_steps: Any = ..., pri_eta: float = ..., default_priority: Optional[Any] = ..., input_priorities: bool = ..., input_priority_shift: Optional[Any] = ..., value_scale_eps: float = ..., updates_per_sync: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...): ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def compute_input_priorities(self, samples: Any): ...
    def loss(self, samples: Any): ...
    def value_scale(self, x: Any): ...
    def inv_value_scale(self, z: Any): ...
