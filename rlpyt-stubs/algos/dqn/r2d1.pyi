from collections import namedtuple
from rlpyt.agents.base import AgentInputs as AgentInputs
from rlpyt.algos.dqn.dqn import DQN as DQN, SamplesToBuffer as SamplesToBuffer
from rlpyt.algos.utils import discount_return_n_step as discount_return_n_step, valid_from_done as valid_from_done
from rlpyt.replays.sequence.frame import AsyncPrioritizedSequenceReplayFrameBuffer as AsyncPrioritizedSequenceReplayFrameBuffer, AsyncUniformSequenceReplayFrameBuffer as AsyncUniformSequenceReplayFrameBuffer, PrioritizedSequenceReplayFrameBuffer as PrioritizedSequenceReplayFrameBuffer, UniformSequenceReplayFrameBuffer as UniformSequenceReplayFrameBuffer
from rlpyt.utils.buffer import buffer_method as buffer_method, buffer_to as buffer_to, torchify_buffer as torchify_buffer
from rlpyt.utils.collections import namedarraytuple as namedarraytuple
from rlpyt.utils.logging import logger as logger
from rlpyt.utils.quick_args import save__init__args as save__init__args
from rlpyt.utils.tensor import select_at_indexes as select_at_indexes, valid_mean as valid_mean
from typing import Any, Optional

OptInfo = namedtuple('OptInfo', ['loss', 'gradNorm', 'tdAbsErr', 'priority'])
SamplesToBufferRnn: Any
PrioritiesSamplesToBuffer: Any

class R2D1(DQN):
    opt_info_fields: Any = ...
    def __init__(self, discount: float = ..., batch_T: int = ..., batch_B: int = ..., warmup_T: int = ..., store_rnn_state_interval: int = ..., min_steps_learn: Any = ..., delta_clip: Optional[Any] = ..., replay_size: Any = ..., replay_ratio: int = ..., target_update_interval: int = ..., n_step_return: int = ..., learning_rate: float = ..., OptimCls: Any = ..., optim_kwargs: Optional[Any] = ..., initial_optim_state_dict: Optional[Any] = ..., clip_grad_norm: float = ..., eps_steps: Any = ..., double_dqn: bool = ..., prioritized_replay: bool = ..., pri_alpha: float = ..., pri_beta_init: float = ..., pri_beta_final: float = ..., pri_beta_steps: Any = ..., pri_eta: float = ..., default_priority: Optional[Any] = ..., input_priorities: bool = ..., input_priority_shift: Optional[Any] = ..., value_scale_eps: float = ..., ReplayBufferCls: Optional[Any] = ..., updates_per_sync: int = ...) -> None: ...
    replay_buffer: Any = ...
    def initialize_replay_buffer(self, examples: Any, batch_spec: Any, async_: bool = ...): ...
    def optimize_agent(self, itr: Any, samples: Optional[Any] = ..., sampler_itr: Optional[Any] = ...): ...
    def samples_to_buffer(self, samples: Any): ...
    def compute_input_priorities(self, samples: Any): ...
    def loss(self, samples: Any): ...
    def value_scale(self, x: Any): ...
    def inv_value_scale(self, z: Any): ...
